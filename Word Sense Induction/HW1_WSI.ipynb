{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_WSI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgBuVCPHpbm-"
      },
      "source": [
        "***Word Sense Induction:***\n",
        "\n",
        "***Methodology***\n",
        "\n",
        "1) Lemmatizing and tagging(PoS) text based on context. (http://lindat.mff.cuni.cz/services/udpipe/) I used online application interface for this task and copied the text back to csv format\\\n",
        "2) Word2vec representation of vector using pre trained model, vector length = 300. For this project I used  ruscorpora_upos_skipgram_300_5_2018 from: https://rusvectores.org/ru/models/ \\\n",
        "3) The function \"fingerprint\" takes the lemmatized and tagged representation of a word and a pre trained corpus as mentioned above and then finds the vector representations of length 300 based on context, I then took the average vector representation of all such vectors in the model's vocabulary to represent the context for our particular semantic use \\\n",
        "4) Taking just the average of the vectors gave the following warning  \"All samples have mutually equal similarities. \" and so I took a weighted average instead where the weight for a particular vector is dependent on the frequency of its occurance\\\n",
        "5) Affinity Propagation produces clustering of the contexts without the redefined number of clusters, which can be used\n",
        "immediately as the desired sense-specific grouping. It takes two parameters for input: damping and preference which can be iteratively optimised but I just took a safe value of 0.5 each for the sake of trying \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq6jIud92A3i"
      },
      "source": [
        "**Running the program:**\n",
        "\n",
        "Please save the below blocks of code in seperate files named wsi.py, helpers.py, evaluate.py respectively and use the following command:  \n",
        "!python3 [wsi path] --input [test_tagged csv path] --model [zipped model path from the above mentioned rus2vec page] --test\n",
        "\n",
        "Since I used the online service for tagging and lemmatization I am attaching the tagged csvs in the zipped folder\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJAsdMtt57Vq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "2f287251-fcdf-4b7e-e236-28b02dac6933"
      },
      "source": [
        "#wsi.py\n",
        "\n",
        "from os import path\n",
        "from pandas import read_csv\n",
        "from evaluate import evaluate\n",
        "import argparse\n",
        "import sys\n",
        "import numpy as np\n",
        "import gensim\n",
        "import logging\n",
        "from sklearn.cluster import AffinityPropagation, SpectralClustering\n",
        "from helpers import fingerprint, save\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    arg = parser.add_argument\n",
        "    arg('--input', help='Path to input file with contexts', required=True)\n",
        "    arg('--model', help='Path to word2vec model', required=True)\n",
        "    arg('--test', dest='testing', action='store_true', help='Make predictions for test file with no gold labels?')\n",
        "    arg('--weights', dest='weights', action='store_true', help='Use word weights?')\n",
        "\n",
        "    parser.set_defaults(testing=False)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    modelfile = args.model\n",
        "\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(modelfile, binary=False)\n",
        "    \n",
        "    model.init_sims(replace=True)\n",
        "    dataset = args.input\n",
        "\n",
        "    # Affinity Cluster Algorithm input parameters damping and preference which could be further optimised but I chose some common values to proceed\n",
        "    damping = 0.5\n",
        "    preference = 0.5\n",
        "\n",
        "    df = read_csv(dataset, sep=\"\\t\", encoding=\"utf-8\")\n",
        "    predicted = []\n",
        "    goldsenses = []\n",
        "    for query in df.word.unique():\n",
        "        #analysing words in the unique word query list\n",
        "        subset = df[df.word == query]\n",
        "        if not args.testing:\n",
        "            goldsenses.append(len(subset.gold_sense_id.unique()))\n",
        "        contexts = []\n",
        "        matrix = np.empty((subset.shape[0], model.vector_size))\n",
        "        counter = 0\n",
        "        lengths = []\n",
        "        for line in subset.iterrows():\n",
        "            con = line[1].context\n",
        "            identifier = line[1].context_id\n",
        "            label = query + str(identifier)\n",
        "            contexts.append(label)\n",
        "            if type(con) == float:\n",
        "                print('Empty context at', label, file=sys.stderr)\n",
        "                fp = np.zeros(model.vector_size)\n",
        "            else:\n",
        "                bow = con.split()\n",
        "                bow = [b for b in bow if b != query]\n",
        "                fp = fingerprint(bow, model, weights=args.weights)\n",
        "                lengths.append(len(bow))\n",
        "            matrix[counter, :] = fp\n",
        "            counter += 1\n",
        "        clustering = AffinityPropagation(preference=preference, damping=damping, random_state=None).fit(matrix)\n",
        "       \n",
        "        cur_predicted = clustering.labels_.tolist()\n",
        "        predicted += cur_predicted\n",
        "        if not args.testing:\n",
        "            gold = subset.gold_sense_id\n",
        "            print('Gold clusters:', len(set(gold)), file=sys.stderr)\n",
        "        print('Predicted clusters:', len(set(cur_predicted)), file=sys.stderr)\n",
        "       \n",
        "\n",
        "    df.predict_sense_id = predicted\n",
        "    fname = path.splitext(path.basename(args.input))[0]\n",
        "    if args.testing:\n",
        "        save(df, fname)\n",
        "    else:\n",
        "        res = evaluate(save(df, fname))\n",
        "        print('ARI:', res)\n",
        "        print('Average number of senses:', np.average(goldsenses))\n",
        "        print('Variation of the number of senses:', np.std(goldsenses))\n",
        "        print('Minimum number of senses:', np.min(goldsenses))\n",
        "        print('Maximum number of senses:', np.max(goldsenses))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] --input INPUT --model MODEL [--test]\n",
            "                             [--weights]\n",
            "ipykernel_launcher.py: error: the following arguments are required: --input, --model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBu0bQ4ES4a1"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuyBvALa6FAk"
      },
      "source": [
        "#helpers.py\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import pylab as plot\n",
        "\n",
        "\"\"\"\n",
        "   Words list\n",
        "   Word2vec model in Gensim format -- Parameter Model   \n",
        "    function returns average vector of words in text\n",
        "  \"\"\"\n",
        "def fingerprint(text, model, weights=False):\n",
        "\n",
        "    # Creating list of all words in the document, which are present in the model\n",
        "    words = [w for w in text if w in model]\n",
        "    lexicon = list(set(words))\n",
        "    l = len(lexicon)\n",
        "    if l < 1:\n",
        "        print('Empty lexicon in', text, file=sys.stderr)\n",
        "        return np.zeros(model.vector_size)\n",
        "    vectors = np.zeros((l, model.vector_size))  # Creating empty matrix of vectors for words\n",
        "    for i in list(range(l)):  # Iterate over words in the text\n",
        "        word = lexicon[i]\n",
        "        if weights:\n",
        "            weight = wordweight(word, model)\n",
        "        else:\n",
        "            weight = 1.0\n",
        "        vectors[i, :] = model[word] * weight  # Adding word and its vector to matrix\n",
        "    semantic_fingerprint = np.sum(vectors, axis=0)  # Computing sum of all vectors in the document\n",
        "    semantic_fingerprint = np.divide(semantic_fingerprint, l)  # Computing average vector\n",
        "    return semantic_fingerprint\n",
        "\n",
        "\n",
        "def wordweight(word, model, a=10 ** -3, w_count=30000000):\n",
        "\n",
        "    prob = model.wv.vocab[word].count / w_count\n",
        "    weight = a / (a + prob)\n",
        "    return weight\n",
        "\n",
        "\n",
        "def save(df, corpus):\n",
        "\n",
        "    #return: path to the saved file\n",
        "\n",
        "    output_fpath = corpus + \"_predictions.csv\"\n",
        "    df.to_csv(output_fpath, sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
        "    print(\"Generated dataset: {}\".format(output_fpath))\n",
        "    return output_fpath\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcoamrdD6IhU"
      },
      "source": [
        "#evaluate.py\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "def gold_predict(df):\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    df['predict'] = df['word'] + '_' + df['predict_sense_id']\n",
        "    df['gold'] = df['word'] + '_' + df['gold_sense_id']\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def ari_per_word_weighted(df):\n",
        "    \"\"\" Computing ARI \"\"\"\n",
        "\n",
        "    df = gold_predict(df)\n",
        "\n",
        "    words = {word: (adjusted_rand_score(df_word.gold, df_word.predict), len(df_word))\n",
        "             for word in df.word.unique()\n",
        "             for df_word in (df.loc[df['word'] == word],)}\n",
        "\n",
        "    cumsum = sum(ari * count for ari, count in words.values())\n",
        "    total = sum(count for _, count in words.values())\n",
        "\n",
        "    r = cumsum/total\n",
        "\n",
        "    return r, words\n",
        "\n",
        "\n",
        "def evaluate(dataset_fpath):\n",
        "    df = read_csv(dataset_fpath, sep='\\t', dtype={'gold_sense_id': str, 'predict_sense_id': str})\n",
        "    ari, words = ari_per_word_weighted(df)\n",
        "    print('{}\\t{}\\t{}'.format('word', 'ari', 'count'))\n",
        "\n",
        "    for word in sorted(words.keys()):\n",
        "        print('{}\\t{:.6f}\\t{:d}'.format(word, *words[word]))\n",
        "\n",
        "    print('\\t{:.6f}\\t{:d}'.format(ari, len(df)))\n",
        "    return ari\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n",
        "\n",
        "    parser.add_argument('dataset', type=argparse.FileType('r'))\n",
        "    args = parser.parse_args()\n",
        "    evaluate(args.dataset)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyOGIso6a9M"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJeQHxJc6bhm"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnBg34vS6djk"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1kQMG2P-qCK45ev1URrnMTD5nQSZxxfR1\"})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5YXQfn-6kmd"
      },
      "source": [
        "downloaded.GetContentFile('ruscorpora_upos_skipgram_300_5_2018.vec.gz')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgISF_Hj6m9A",
        "outputId": "e0e1ac42-8545-4c3a-ce32-0e45d52f983c"
      },
      "source": [
        "!python3 /content/wsi.py --input /content/test_tagged.csv --model /content/ruscorpora_upos_skipgram_300_5_2018.vec.gz --test"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-28 08:48:02,982 : INFO : loading projection weights from /content/ruscorpora_upos_skipgram_300_5_2018.vec.gz\n",
            "2021-11-28 08:49:08,647 : INFO : loaded (195071, 300) matrix from /content/ruscorpora_upos_skipgram_300_5_2018.vec.gz\n",
            "2021-11-28 08:49:08,647 : INFO : precomputing L2-norms of word weight vectors\n",
            "Predicted clusters: 112\n",
            "Predicted clusters: 135\n",
            "Predicted clusters: 84\n",
            "Predicted clusters: 60\n",
            "Predicted clusters: 20\n",
            "Predicted clusters: 148\n",
            "Predicted clusters: 79\n",
            "Generated dataset: test_tagged_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_fMtmPy4R9f"
      },
      "source": [
        "**Results and Discussion**\n",
        "\n"
      ]
    }
  ]
}