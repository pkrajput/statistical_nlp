{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxonomy_Induction_en.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Introduction**\n",
        "\n",
        "Volabulary in a language is always evolving, new words get introduced as society progresses and to relate these new words to classes in a Thesaurus manually is just too slow, The task of taxonomy induction takes in these new words and attaches in a heirarchical graph with similar meanings. I chose to use Word2Vec and Node2Vec embeddings for this task though it can also be approached via transformer based parsers like Bert. As My understanding of Russian Language and thus vocabulary is close to null :) I chose to do this task for english. The suggested competitiion [SemEval-2015 Task 17](https://alt.qcri.org/semeval2015/task17/) though has no test set for such a task. It is locked. I asked help in the Telegram grp for help but no one responded, Since it is the last assignment and everyone is in a festive mood I would like to use the test data from seminar demonstration for this task, hoping I will be excused. I would also like to mention that I found our TA's paper on the same [Studying Taxonomy Enrichment on Diachronic WordNet Versions](https://arxiv.org/abs/2011.11536) but when I looked for the data set I again could not find it [paper dataset](https://paperswithcode.com/paper/studying-taxonomy-enrichment-on-diachronic). I hope this notebook will provide some proof of concept of my understanding of the task and following of the lectures. "
      ],
      "metadata": {
        "id": "qBq3wRTlG7Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Methodology**\n",
        "\n",
        "1) We first train the Word2vec corpus using random walks on nouns with walk length = 30 and 20 such walks in parallel with total walks = 200.\\\n",
        "2) We then use Word2Vec and this trained corpus to create our own Node2Vec.\\\n",
        "3) We then import nouns_w2v_all_2.0-3.0.txt file which has out of volabulary words and use our trained Node2Vec model to find mutual lemmas to create a dictionary\\\n",
        "4) We use this dictionary to create a transform from Word2Vec to Node2Vec and find our Node2Vec representations of the OOV words\\\n",
        "5) We then import our evaluate function and test set with reference hypernyms and the same unmarked words\\\n",
        "6) We convert out unmarked words to Node2Vec and evaluate the mAP score, mRR score\\\n",
        "7) I also used nouns_w2v_all_2.0-3.0.txt Word2Vec embedding to calculate the similar embeddings for OOV words and run the evaluate function again\\\n",
        "8) At last eh HCHModel function is used to find the similarity words(associates) and top 10 hypernyms of similarity words.\n",
        "\n"
      ],
      "metadata": {
        "id": "osSrJFtgKgbZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7mIumTUrC8Wa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from joblib import Parallel, delayed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProbabilityGenerator:\n",
        "    def __init__(self, p_return_param, q_unseen_param):\n",
        "        self.p_return_param = p_return_param\n",
        "        self.q_unseen_param = q_unseen_param\n",
        "\n",
        "    def generate_probabilities(self, source_id, prev_value, previous_starts, possible_starts):\n",
        "        result_weights = self.__compute_weights(source_id, prev_value, \n",
        "                                                previous_starts, \n",
        "                                                possible_starts)\n",
        "        sum_weights = sum(result_weights)\n",
        "        if sum_weights == 0:\n",
        "            return None\n",
        "        else:\n",
        "            return [weight / sum_weights for weight in result_weights]\n",
        "\n",
        "    def __compute_weights(self, source_id, prev_value, previous_starts, \n",
        "                          possible_starts):\n",
        "        if prev_value:\n",
        "            weights = [self.__compute_weight(source_id, possible_start, \n",
        "                                             prev_value, previous_starts)\n",
        "                for possible_start in possible_starts]\n",
        "        else:  # equal weights\n",
        "            weights = [1]*len(possible_starts)\n",
        "        \n",
        "        return weights\n",
        "\n",
        "    def __compute_weight(self, source_id, possible_start, prev_value, previous_starts):\n",
        "        if possible_start == source_id:\n",
        "            # equals to node one step before\n",
        "            return 1 / self.p_return_param\n",
        "        elif possible_start in previous_starts or possible_start==prev_value:\n",
        "            # is neighbour of previous node\n",
        "            return 1\n",
        "        else:\n",
        "            # not seen yet\n",
        "            return 1 / self.q_unseen_param"
      ],
      "metadata": {
        "id": "YIpbwoAADGDM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_node(possible_nodes, probabilities):\n",
        "    random_index = np.random.choice(len(possible_nodes), p=probabilities)\n",
        "    chosen_link_with_anchor = possible_nodes[random_index]\n",
        "    return chosen_link_with_anchor"
      ],
      "metadata": {
        "id": "PyhIQciBDLAS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def go_for_a_walk(node, walk_length, prob_generator):\n",
        "    source_id = node\n",
        "    prev_start = None\n",
        "    prev_possible_nodes = []\n",
        "    sequence = [source_id]\n",
        "    \n",
        "    while len(sequence) < walk_length:\n",
        "        possible_nodes = list(G.neighbors(source_id))\n",
        "        if possible_nodes:\n",
        "            probabilities = prob_generator.generate_probabilities(source_id, prev_start, \n",
        "                                                   prev_possible_nodes,\n",
        "                                                   possible_nodes)\n",
        "            if probabilities:\n",
        "                    chosen_node = choose_node(possible_nodes, probabilities)\n",
        "                    prev_start = source_id\n",
        "                    prev_possible_nodes = possible_nodes\n",
        "                    source_id = chosen_node\n",
        "                    sequence.append(source_id)\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "tf8p7HGnDNsa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_walk(graph, walk_length, num_walks, prob_gen):\n",
        "    walks = []\n",
        "    for n_walk in tqdm(range(num_walks)):\n",
        "        shuffled_nodes = list(graph.nodes())\n",
        "        random.shuffle(shuffled_nodes)\n",
        "        for source in shuffled_nodes:\n",
        "            walks.append(go_for_a_walk(source, walk_length, prob_gen))\n",
        "    return walks"
      ],
      "metadata": {
        "id": "PC07cWKkDQXW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def build_graph(wordnet, pos, directed=False):\n",
        "    if directed:\n",
        "        G = nx.DiGraph()\n",
        "    else:\n",
        "        G = nx.Graph()\n",
        "    for synset in wordnet.all_synsets(pos):\n",
        "        for hypernym in synset.hypernyms():\n",
        "            G.add_edge(synset.name(), hypernym.name())\n",
        "        if len(synset.hypernyms()) == 0:\n",
        "            G.add_node(synset.name())\n",
        "    return G"
      ],
      "metadata": {
        "id": "Lk3w8JavDSfj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://wordnetcode.princeton.edu/2.0/WordNet-2.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p1vBuj-DUkc",
        "outputId": "689fd958-ef4a-4e79-d5f3-e05031310d57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 10:59:52--  http://wordnetcode.princeton.edu/2.0/WordNet-2.0.tar.gz\n",
            "Resolving wordnetcode.princeton.edu (wordnetcode.princeton.edu)... 128.112.136.61\n",
            "Connecting to wordnetcode.princeton.edu (wordnetcode.princeton.edu)|128.112.136.61|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://wordnetcode.princeton.edu/2.0/WordNet-2.0.tar.gz [following]\n",
            "--2021-12-24 10:59:53--  https://wordnetcode.princeton.edu/2.0/WordNet-2.0.tar.gz\n",
            "Connecting to wordnetcode.princeton.edu (wordnetcode.princeton.edu)|128.112.136.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12847598 (12M) [application/x-gzip]\n",
            "Saving to: ‘WordNet-2.0.tar.gz’\n",
            "\n",
            "WordNet-2.0.tar.gz  100%[===================>]  12.25M  5.98MB/s    in 2.0s    \n",
            "\n",
            "2021-12-24 10:59:55 (5.98 MB/s) - ‘WordNet-2.0.tar.gz’ saved [12847598/12847598]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf WordNet-2.0.tar.gz"
      ],
      "metadata": {
        "id": "lcc8MrI9Dy6G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import WordNetCorpusReader"
      ],
      "metadata": {
        "id": "MZSdc7-cGSI6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn = WordNetCorpusReader('./WordNet-2.0/dict', None)"
      ],
      "metadata": {
        "id": "ilaESkYJGW8r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POS = 'n'\n",
        "G = build_graph(wn, POS, False)"
      ],
      "metadata": {
        "id": "2to5EuK1GaDe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "left to train via random walks for 1hr 20mins still didn't finish"
      ],
      "metadata": {
        "id": "SMTzGSlqYr5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prob_gen = ProbabilityGenerator(p_return_param=4, q_unseen_param=0.5)\n",
        "# num_walks = 200\n",
        "# walk_length = 30\n",
        "# n_workers = 20\n",
        "\n",
        "# num_walks_lists = np.array_split(range(num_walks), n_workers)\n",
        "# flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "# walk_results = Parallel(n_jobs=n_workers, temp_folder=None, require=None, verbose=200)(\n",
        "#         delayed(parallel_walk)(G, walk_length, len(num_walks), prob_gen) for idx, \n",
        "#         num_walks in enumerate(num_walks_lists))\n",
        "\n",
        "# walks = flatten(walk_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "oTHcvNGzGdEU",
        "outputId": "d629e79b-2512-4aa9-9ab7-e22f78149df0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed: 77.1min\n",
            "[Parallel(n_jobs=20)]: Done   2 out of  20 | elapsed: 77.1min remaining: 694.0min\n",
            "[Parallel(n_jobs=20)]: Done   3 out of  20 | elapsed: 77.1min remaining: 437.0min\n",
            "[Parallel(n_jobs=20)]: Done   4 out of  20 | elapsed: 77.1min remaining: 308.5min\n",
            "[Parallel(n_jobs=20)]: Done   5 out of  20 | elapsed: 77.1min remaining: 231.3min\n",
            "[Parallel(n_jobs=20)]: Done   6 out of  20 | elapsed: 77.1min remaining: 179.9min\n",
            "[Parallel(n_jobs=20)]: Done   7 out of  20 | elapsed: 77.1min remaining: 143.2min\n",
            "[Parallel(n_jobs=20)]: Done   8 out of  20 | elapsed: 77.1min remaining: 115.7min\n",
            "[Parallel(n_jobs=20)]: Done   9 out of  20 | elapsed: 77.1min remaining: 94.2min\n",
            "[Parallel(n_jobs=20)]: Done  10 out of  20 | elapsed: 77.1min remaining: 77.1min\n",
            "[Parallel(n_jobs=20)]: Done  11 out of  20 | elapsed: 77.1min remaining: 63.1min\n",
            "[Parallel(n_jobs=20)]: Done  12 out of  20 | elapsed: 77.1min remaining: 51.4min\n",
            "[Parallel(n_jobs=20)]: Done  13 out of  20 | elapsed: 77.1min remaining: 41.5min\n",
            "[Parallel(n_jobs=20)]: Done  14 out of  20 | elapsed: 77.1min remaining: 33.0min\n",
            "[Parallel(n_jobs=20)]: Done  15 out of  20 | elapsed: 77.1min remaining: 25.7min\n",
            "[Parallel(n_jobs=20)]: Done  16 out of  20 | elapsed: 77.1min remaining: 19.3min\n",
            "[Parallel(n_jobs=20)]: Done  17 out of  20 | elapsed: 77.1min remaining: 13.6min\n",
            "[Parallel(n_jobs=20)]: Done  18 out of  20 | elapsed: 77.1min remaining:  8.6min\n",
            "[Parallel(n_jobs=20)]: Done  20 out of  20 | elapsed: 77.1min remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b4c97f587cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m walk_results = Parallel(n_jobs=n_workers, temp_folder=None, require=None, verbose=200)(\n\u001b[1;32m     10\u001b[0m         \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_walk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_walks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_gen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         num_walks in enumerate(num_walks_lists))\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwalks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU\" -O noun_random_walks.pkl && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU3vOYoOGieE",
        "outputId": "34a37c26-a585-4197-a6de-2553adfc66dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 12:21:48--  https://docs.google.com/uc?export=download&confirm=y4UH&id=1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU\n",
            "Resolving docs.google.com (docs.google.com)... 64.233.187.102, 64.233.187.139, 64.233.187.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.187.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-9s-docs.googleusercontent.com/docs/securesc/2vhhujfvaorqgiuf2mtr2osvng92c0gk/23cqap5uf5c1vtcifrjc6283udgp80p9/1640348475000/06548341279621459266/12347628958155703582Z/1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU?e=download [following]\n",
            "--2021-12-24 12:21:49--  https://doc-0o-9s-docs.googleusercontent.com/docs/securesc/2vhhujfvaorqgiuf2mtr2osvng92c0gk/23cqap5uf5c1vtcifrjc6283udgp80p9/1640348475000/06548341279621459266/12347628958155703582Z/1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU?e=download\n",
            "Resolving doc-0o-9s-docs.googleusercontent.com (doc-0o-9s-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-0o-9s-docs.googleusercontent.com (doc-0o-9s-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=51of4aa0r32k4&continue=https://doc-0o-9s-docs.googleusercontent.com/docs/securesc/2vhhujfvaorqgiuf2mtr2osvng92c0gk/23cqap5uf5c1vtcifrjc6283udgp80p9/1640348475000/06548341279621459266/12347628958155703582Z/1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU?e%3Ddownload&hash=1v0i273d4iiuf55ujo3gc4bg9q1tg8qk [following]\n",
            "--2021-12-24 12:21:49--  https://docs.google.com/nonceSigner?nonce=51of4aa0r32k4&continue=https://doc-0o-9s-docs.googleusercontent.com/docs/securesc/2vhhujfvaorqgiuf2mtr2osvng92c0gk/23cqap5uf5c1vtcifrjc6283udgp80p9/1640348475000/06548341279621459266/12347628958155703582Z/1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU?e%3Ddownload&hash=1v0i273d4iiuf55ujo3gc4bg9q1tg8qk\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.187.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0o-9s-docs.googleusercontent.com/docs/securesc/2vhhujfvaorqgiuf2mtr2osvng92c0gk/23cqap5uf5c1vtcifrjc6283udgp80p9/1640348475000/06548341279621459266/12347628958155703582Z/1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU?e=download&nonce=51of4aa0r32k4&user=12347628958155703582Z&hash=f2jueqf67ggdva5v6vod1ke1b0akq5cp [following]\n",
            "--2021-12-24 12:21:50--  https://doc-0o-9s-docs.googleusercontent.com/docs/securesc/2vhhujfvaorqgiuf2mtr2osvng92c0gk/23cqap5uf5c1vtcifrjc6283udgp80p9/1640348475000/06548341279621459266/12347628958155703582Z/1rBgK260arEeUe42oyJ8n_PfVqHJUIKFU?e=download&nonce=51of4aa0r32k4&user=12347628958155703582Z&hash=f2jueqf67ggdva5v6vod1ke1b0akq5cp\n",
            "Connecting to doc-0o-9s-docs.googleusercontent.com (doc-0o-9s-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2549658219 (2.4G) [application/octet-stream]\n",
            "Saving to: ‘noun_random_walks.pkl’\n",
            "\n",
            "noun_random_walks.p 100%[===================>]   2.37G  78.1MB/s    in 34s     \n",
            "\n",
            "2021-12-24 12:22:25 (70.6 MB/s) - ‘noun_random_walks.pkl’ saved [2549658219/2549658219]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"noun_random_walks.pkl\", 'rb') as f:\n",
        "    random_walks = pickle.load(f)"
      ],
      "metadata": {
        "id": "Gy3gXCdjY9E0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models import Word2Vec\n",
        "# sg_model = Word2Vec(random_walks, min_count=0, workers=100, negative=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "-HqFhtsHZ_fy",
        "outputId": "c451f382-c0d1-480b-c06f-453b4af35562"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-67ab6207646e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again coudl't finish within 30mins and lead to RAM overflow multiple times"
      ],
      "metadata": {
        "id": "BZDnCsB5frze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1IcjBFgr011eEkefo6kMANAUSDhS6Otyk' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1IcjBFgr011eEkefo6kMANAUSDhS6Otyk\" -O gensim_node2vec.txt && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbSs7cKjad4r",
        "outputId": "1616b20a-4f46-4700-f59c-4f06d4bb844f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 12:52:24--  https://docs.google.com/uc?export=download&confirm=yS48&id=1IcjBFgr011eEkefo6kMANAUSDhS6Otyk\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.204.100, 74.125.204.139, 74.125.204.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-1g-docs.googleusercontent.com/docs/securesc/g0aq72hut5dtu23km53kdo4ogj8itgaj/9nd7i0d1g0jlpmlk4qnb4ga80on0ttnb/1640350275000/06548341279621459266/08203973431494720941Z/1IcjBFgr011eEkefo6kMANAUSDhS6Otyk?e=download [following]\n",
            "--2021-12-24 12:52:24--  https://doc-00-1g-docs.googleusercontent.com/docs/securesc/g0aq72hut5dtu23km53kdo4ogj8itgaj/9nd7i0d1g0jlpmlk4qnb4ga80on0ttnb/1640350275000/06548341279621459266/08203973431494720941Z/1IcjBFgr011eEkefo6kMANAUSDhS6Otyk?e=download\n",
            "Resolving doc-00-1g-docs.googleusercontent.com (doc-00-1g-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-00-1g-docs.googleusercontent.com (doc-00-1g-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=mqkh6r1603eqs&continue=https://doc-00-1g-docs.googleusercontent.com/docs/securesc/g0aq72hut5dtu23km53kdo4ogj8itgaj/9nd7i0d1g0jlpmlk4qnb4ga80on0ttnb/1640350275000/06548341279621459266/08203973431494720941Z/1IcjBFgr011eEkefo6kMANAUSDhS6Otyk?e%3Ddownload&hash=l28ci0piiogtn86vc9t1mei2frltkie7 [following]\n",
            "--2021-12-24 12:52:24--  https://docs.google.com/nonceSigner?nonce=mqkh6r1603eqs&continue=https://doc-00-1g-docs.googleusercontent.com/docs/securesc/g0aq72hut5dtu23km53kdo4ogj8itgaj/9nd7i0d1g0jlpmlk4qnb4ga80on0ttnb/1640350275000/06548341279621459266/08203973431494720941Z/1IcjBFgr011eEkefo6kMANAUSDhS6Otyk?e%3Ddownload&hash=l28ci0piiogtn86vc9t1mei2frltkie7\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-00-1g-docs.googleusercontent.com/docs/securesc/g0aq72hut5dtu23km53kdo4ogj8itgaj/9nd7i0d1g0jlpmlk4qnb4ga80on0ttnb/1640350275000/06548341279621459266/08203973431494720941Z/1IcjBFgr011eEkefo6kMANAUSDhS6Otyk?e=download&nonce=mqkh6r1603eqs&user=08203973431494720941Z&hash=qd7l156nsrdqtfs664gik9c0f632o5is [following]\n",
            "--2021-12-24 12:52:25--  https://doc-00-1g-docs.googleusercontent.com/docs/securesc/g0aq72hut5dtu23km53kdo4ogj8itgaj/9nd7i0d1g0jlpmlk4qnb4ga80on0ttnb/1640350275000/06548341279621459266/08203973431494720941Z/1IcjBFgr011eEkefo6kMANAUSDhS6Otyk?e=download&nonce=mqkh6r1603eqs&user=08203973431494720941Z&hash=qd7l156nsrdqtfs664gik9c0f632o5is\n",
            "Connecting to doc-00-1g-docs.googleusercontent.com (doc-00-1g-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 272657482 (260M) [text/plain]\n",
            "Saving to: ‘gensim_node2vec.txt’\n",
            "\n",
            "gensim_node2vec.txt 100%[===================>] 260.03M  61.2MB/s    in 4.2s    \n",
            "\n",
            "2021-12-24 12:52:30 (61.2 MB/s) - ‘gensim_node2vec.txt’ saved [272657482/272657482]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "sg_model = KeyedVectors.load_word2vec_format(\"gensim_node2vec.txt\")"
      ],
      "metadata": {
        "id": "MNhPBqiEf6vK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sg_model.wv.similar_by_vector(\"train.n.01\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNZCQbZ1gC0S",
        "outputId": "5bfd2ebb-0acd-4279-8b7e-e3df85fe0b77"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('freight_liner.n.01', 0.9477183818817139),\n",
              " ('freight_train.n.01', 0.9414694905281067),\n",
              " ('bullet_train.n.01', 0.9110889434814453),\n",
              " ('passenger_train.n.01', 0.9105428457260132),\n",
              " ('commuter.n.01', 0.8963304758071899),\n",
              " ('mail_train.n.01', 0.8922033905982971),\n",
              " ('hospital_train.n.01', 0.8852114081382751),\n",
              " ('car_train.n.01', 0.8846890330314636),\n",
              " ('subway_train.n.01', 0.8838037252426147),\n",
              " ('streamliner.n.01', 0.8825638294219971)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X\" -O nouns_w2v_all_2.0-3.0.txt && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykJ3iZWbhaVG",
        "outputId": "aa61ddd9-b775-4ff1-aefa-c897842d6790"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 12:59:04--  https://docs.google.com/uc?export=download&confirm=rBou&id=14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X\n",
            "Resolving docs.google.com (docs.google.com)... 64.233.188.138, 64.233.188.102, 64.233.188.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.188.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-ag-docs.googleusercontent.com/docs/securesc/obens7ffvs9fc9na3ftc1fa1ni7odj25/g8ake993v0d7e6b7sh6p508h5nm7e3d1/1640350725000/06548341279621459266/08097295962724454176Z/14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X?e=download [following]\n",
            "--2021-12-24 12:59:04--  https://doc-0s-ag-docs.googleusercontent.com/docs/securesc/obens7ffvs9fc9na3ftc1fa1ni7odj25/g8ake993v0d7e6b7sh6p508h5nm7e3d1/1640350725000/06548341279621459266/08097295962724454176Z/14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X?e=download\n",
            "Resolving doc-0s-ag-docs.googleusercontent.com (doc-0s-ag-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-0s-ag-docs.googleusercontent.com (doc-0s-ag-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=dkjobc45vtqgi&continue=https://doc-0s-ag-docs.googleusercontent.com/docs/securesc/obens7ffvs9fc9na3ftc1fa1ni7odj25/g8ake993v0d7e6b7sh6p508h5nm7e3d1/1640350725000/06548341279621459266/08097295962724454176Z/14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X?e%3Ddownload&hash=c3fj86cq8c6qfn5b1t3u97kt462c81i7 [following]\n",
            "--2021-12-24 12:59:04--  https://docs.google.com/nonceSigner?nonce=dkjobc45vtqgi&continue=https://doc-0s-ag-docs.googleusercontent.com/docs/securesc/obens7ffvs9fc9na3ftc1fa1ni7odj25/g8ake993v0d7e6b7sh6p508h5nm7e3d1/1640350725000/06548341279621459266/08097295962724454176Z/14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X?e%3Ddownload&hash=c3fj86cq8c6qfn5b1t3u97kt462c81i7\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.188.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0s-ag-docs.googleusercontent.com/docs/securesc/obens7ffvs9fc9na3ftc1fa1ni7odj25/g8ake993v0d7e6b7sh6p508h5nm7e3d1/1640350725000/06548341279621459266/08097295962724454176Z/14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X?e=download&nonce=dkjobc45vtqgi&user=08097295962724454176Z&hash=v319eohsmkvoh7e6bq5h07eotc3hncnh [following]\n",
            "--2021-12-24 12:59:05--  https://doc-0s-ag-docs.googleusercontent.com/docs/securesc/obens7ffvs9fc9na3ftc1fa1ni7odj25/g8ake993v0d7e6b7sh6p508h5nm7e3d1/1640350725000/06548341279621459266/08097295962724454176Z/14K2HKaa1Qg5MJzumyX2wPUqvWfHJhX8X?e=download&nonce=dkjobc45vtqgi&user=08097295962724454176Z&hash=v319eohsmkvoh7e6bq5h07eotc3hncnh\n",
            "Connecting to doc-0s-ag-docs.googleusercontent.com (doc-0s-ag-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 517809558 (494M) [text/plain]\n",
            "Saving to: ‘nouns_w2v_all_2.0-3.0.txt’\n",
            "\n",
            "nouns_w2v_all_2.0-3 100%[===================>] 493.82M   146MB/s    in 3.4s    \n",
            "\n",
            "2021-12-24 12:59:09 (144 MB/s) - ‘nouns_w2v_all_2.0-3.0.txt’ saved [517809558/517809558]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2==0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
        "    \"\"\"\n",
        "    Source and target dictionaries are the FastVector objects of\n",
        "    source/target languages. bilingual_dictionary is a list of \n",
        "    translation pair tuples [(source_word, target_word), ...].\n",
        "    \"\"\"\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "\n",
        "    for (source, target) in bilingual_dictionary:\n",
        "        if source in source_dictionary and target in target_dictionary:\n",
        "            source_matrix.append(source_dictionary[source])\n",
        "            target_matrix.append(target_dictionary[target])\n",
        "\n",
        "    # return training matrices\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n",
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
        "    \"\"\"\n",
        "    Source and target matrices are numpy arrays, shape\n",
        "    (dictionary_length, embedding_dimension). These contain paired\n",
        "    word vectors from the bilingual dictionary.\n",
        "    \"\"\"\n",
        "    # optionally normalize the training vectors\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalized(source_matrix)\n",
        "        target_matrix = normalized(target_matrix)\n",
        "\n",
        "    # perform the SVD\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    U, s, V = np.linalg.svd(product)\n",
        "\n",
        "    # return orthogonal transformation which aligns source language to the target\n",
        "    return np.matmul(U, V)"
      ],
      "metadata": {
        "id": "R77Xtkd8hgDy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastVector:\n",
        "    def __init__(self, vector_file='', transform=None):\n",
        "        self.word2id = {}\n",
        "        self.id2word = []\n",
        "\n",
        "        print('reading word vectors from %s' % vector_file)\n",
        "        with open(vector_file, 'r', encoding=\"utf-8\") as f:\n",
        "            (self.n_words, self.n_dim) = \\\n",
        "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
        "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
        "            for i, line in enumerate(f):\n",
        "                elems = line.rstrip('\\n').split(' ')\n",
        "                self.word2id[elems[0]] = i\n",
        "                self.embed[i] = elems[1:self.n_dim+1]\n",
        "                self.id2word.append(elems[0])\n",
        "\n",
        "        if transform is not None:\n",
        "            print('Applying transformation to embedding')\n",
        "            self.apply_transform(transform)\n",
        "\n",
        "    def apply_transform(self, transform):\n",
        "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
        "        self.embed = np.matmul(self.embed, transmat)\n",
        "\n",
        "    def export(self, outpath):\n",
        "\n",
        "        fout = open(outpath, \"w\")\n",
        "\n",
        "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
        "        for token in self.id2word:\n",
        "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
        "            vector_as_string = \" \".join(vector_components)\n",
        "\n",
        "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
        "            fout.write(out_line)\n",
        "\n",
        "        fout.close()\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def cosine_similarity(cls, vec_a, vec_b):\n",
        "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
        "        return np.dot(vec_a, vec_b) / \\\n",
        "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return key in self.word2id\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.embed[self.word2id[key]]"
      ],
      "metadata": {
        "id": "315y_irlhkqW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_vectors = FastVector(vector_file=\"./nouns_w2v_all_2.0-3.0.txt\")\n",
        "node2vec_vectors = FastVector(vector_file=\"gensim_node2vec.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LgFvYyNmyM8",
        "outputId": "eeab9a4b-cc7c-4234-803a-a51ce0e16d60"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading word vectors from ./nouns_w2v_all_2.0-3.0.txt\n",
            "reading word vectors from gensim_node2vec.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_lemmas(synset):\n",
        "    final_lemmas = []\n",
        "    for lemma in synset.lemmas():\n",
        "        if len(wn.synsets(lemma.name())) == 1 and \"_\" not in lemma.name():\n",
        "            final_lemmas.append(lemma.name())\n",
        "    return final_lemmas"
      ],
      "metadata": {
        "id": "kjLybHp6m1oW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_lemmas = [i.name() for i in wn.all_synsets(POS) if len(check_lemmas(i)) > 0]\n",
        "bilingual_dictionary = [(entry, entry) for entry in common_lemmas]\n",
        "print(len(bilingual_dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEx5N8woshNz",
        "outputId": "01150d1a-621b-42ed-ceb3-cf250a99c245"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_matrix, trg_matrix = make_training_matrices(word2vec_vectors, node2vec_vectors, bilingual_dictionary)"
      ],
      "metadata": {
        "id": "72URe5KmtO1x"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = learn_transformation(src_matrix, trg_matrix)\n",
        "word2vec_vectors.apply_transform(transform)"
      ],
      "metadata": {
        "id": "LM4h_8xtsif8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB\" -O nouns_en.2.0-3.0.tsv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H\" -O no_labels_nouns_en.2.0-3.0.tsv && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL00ucOjtK2G",
        "outputId": "270711b9-6b32-46cc-bae4-c48a8298b1f6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 13:57:25--  https://docs.google.com/uc?export=download&confirm=&id=1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB\n",
            "Resolving docs.google.com (docs.google.com)... 64.233.189.100, 64.233.189.138, 64.233.189.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.189.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-1g-docs.googleusercontent.com/docs/securesc/vsj7dnt08d55jl6mq53n7f1adv4dqvcj/86o8344onvbklcknppu6klobu1rtfsvb/1640354175000/06548341279621459266/04175200007555094289Z/1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB?e=download [following]\n",
            "--2021-12-24 13:57:26--  https://doc-10-1g-docs.googleusercontent.com/docs/securesc/vsj7dnt08d55jl6mq53n7f1adv4dqvcj/86o8344onvbklcknppu6klobu1rtfsvb/1640354175000/06548341279621459266/04175200007555094289Z/1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB?e=download\n",
            "Resolving doc-10-1g-docs.googleusercontent.com (doc-10-1g-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-10-1g-docs.googleusercontent.com (doc-10-1g-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=a2j7bqhnlkhdm&continue=https://doc-10-1g-docs.googleusercontent.com/docs/securesc/vsj7dnt08d55jl6mq53n7f1adv4dqvcj/86o8344onvbklcknppu6klobu1rtfsvb/1640354175000/06548341279621459266/04175200007555094289Z/1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB?e%3Ddownload&hash=q8bo9q42e80dl1h3c0mn68fusal2g1u2 [following]\n",
            "--2021-12-24 13:57:26--  https://docs.google.com/nonceSigner?nonce=a2j7bqhnlkhdm&continue=https://doc-10-1g-docs.googleusercontent.com/docs/securesc/vsj7dnt08d55jl6mq53n7f1adv4dqvcj/86o8344onvbklcknppu6klobu1rtfsvb/1640354175000/06548341279621459266/04175200007555094289Z/1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB?e%3Ddownload&hash=q8bo9q42e80dl1h3c0mn68fusal2g1u2\n",
            "Connecting to docs.google.com (docs.google.com)|64.233.189.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-10-1g-docs.googleusercontent.com/docs/securesc/vsj7dnt08d55jl6mq53n7f1adv4dqvcj/86o8344onvbklcknppu6klobu1rtfsvb/1640354175000/06548341279621459266/04175200007555094289Z/1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB?e=download&nonce=a2j7bqhnlkhdm&user=04175200007555094289Z&hash=7r5kj7qnadhueb4093rp87qiqjl9ahlf [following]\n",
            "--2021-12-24 13:57:26--  https://doc-10-1g-docs.googleusercontent.com/docs/securesc/vsj7dnt08d55jl6mq53n7f1adv4dqvcj/86o8344onvbklcknppu6klobu1rtfsvb/1640354175000/06548341279621459266/04175200007555094289Z/1qR-ujvbRe2LW-KS54PyhVQeSMrbAmMQB?e=download&nonce=a2j7bqhnlkhdm&user=04175200007555094289Z&hash=7r5kj7qnadhueb4093rp87qiqjl9ahlf\n",
            "Connecting to doc-10-1g-docs.googleusercontent.com (doc-10-1g-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46012 (45K) [text/tab-separated-values]\n",
            "Saving to: ‘nouns_en.2.0-3.0.tsv’\n",
            "\n",
            "nouns_en.2.0-3.0.ts 100%[===================>]  44.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-24 13:57:26 (101 MB/s) - ‘nouns_en.2.0-3.0.tsv’ saved [46012/46012]\n",
            "\n",
            "--2021-12-24 13:57:29--  https://docs.google.com/uc?export=download&confirm=&id=18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.8.139, 142.251.8.102, 142.251.8.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.8.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/3ngmdecvosau2nt4jg88q79no7cbbchj/mrhj7s3r392fmhab55lv7kqcreb8cv64/1640354175000/06548341279621459266/03730248515949302132Z/18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H?e=download [following]\n",
            "--2021-12-24 13:57:29--  https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/3ngmdecvosau2nt4jg88q79no7cbbchj/mrhj7s3r392fmhab55lv7kqcreb8cv64/1640354175000/06548341279621459266/03730248515949302132Z/18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H?e=download\n",
            "Resolving doc-0g-2o-docs.googleusercontent.com (doc-0g-2o-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-0g-2o-docs.googleusercontent.com (doc-0g-2o-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=5abt88m1j0ppc&continue=https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/3ngmdecvosau2nt4jg88q79no7cbbchj/mrhj7s3r392fmhab55lv7kqcreb8cv64/1640354175000/06548341279621459266/03730248515949302132Z/18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H?e%3Ddownload&hash=9uiihp8q4paq1am87omk4aidehf5tkda [following]\n",
            "--2021-12-24 13:57:29--  https://docs.google.com/nonceSigner?nonce=5abt88m1j0ppc&continue=https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/3ngmdecvosau2nt4jg88q79no7cbbchj/mrhj7s3r392fmhab55lv7kqcreb8cv64/1640354175000/06548341279621459266/03730248515949302132Z/18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H?e%3Ddownload&hash=9uiihp8q4paq1am87omk4aidehf5tkda\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.8.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/3ngmdecvosau2nt4jg88q79no7cbbchj/mrhj7s3r392fmhab55lv7kqcreb8cv64/1640354175000/06548341279621459266/03730248515949302132Z/18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H?e=download&nonce=5abt88m1j0ppc&user=03730248515949302132Z&hash=80fad2igl6eni2umfoceka1qobj95as6 [following]\n",
            "--2021-12-24 13:57:30--  https://doc-0g-2o-docs.googleusercontent.com/docs/securesc/3ngmdecvosau2nt4jg88q79no7cbbchj/mrhj7s3r392fmhab55lv7kqcreb8cv64/1640354175000/06548341279621459266/03730248515949302132Z/18efUTnAA2ZhhZC2i3_hR_4UX-CfmbN6H?e=download&nonce=5abt88m1j0ppc&user=03730248515949302132Z&hash=80fad2igl6eni2umfoceka1qobj95as6\n",
            "Connecting to doc-0g-2o-docs.googleusercontent.com (doc-0g-2o-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9146 (8.9K) [text/tab-separated-values]\n",
            "Saving to: ‘no_labels_nouns_en.2.0-3.0.tsv’\n",
            "\n",
            "no_labels_nouns_en. 100%[===================>]   8.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-24 13:57:30 (37.6 MB/s) - ‘no_labels_nouns_en.2.0-3.0.tsv’ saved [9146/9146]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1phKUhamxVSa8f68FKgs5lOIUQMnlt58l' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1phKUhamxVSa8f68FKgs5lOIUQMnlt58l\" -O evaluate.py && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-ZuziFyu2oO",
        "outputId": "aba756a0-4b4f-401d-ceda-d9f8fde28ebc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-24 13:58:09--  https://docs.google.com/uc?export=download&confirm=pc_M&id=1phKUhamxVSa8f68FKgs5lOIUQMnlt58l\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.204.101, 74.125.204.139, 74.125.204.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-24-docs.googleusercontent.com/docs/securesc/n4i0jccn2khd3a24fmlud00hma5lfk85/ns4aak217ov5hek6mu4t7shc5d86meec/1640354250000/06548341279621459266/07306670677730082107Z/1phKUhamxVSa8f68FKgs5lOIUQMnlt58l?e=download [following]\n",
            "--2021-12-24 13:58:09--  https://doc-0c-24-docs.googleusercontent.com/docs/securesc/n4i0jccn2khd3a24fmlud00hma5lfk85/ns4aak217ov5hek6mu4t7shc5d86meec/1640354250000/06548341279621459266/07306670677730082107Z/1phKUhamxVSa8f68FKgs5lOIUQMnlt58l?e=download\n",
            "Resolving doc-0c-24-docs.googleusercontent.com (doc-0c-24-docs.googleusercontent.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-0c-24-docs.googleusercontent.com (doc-0c-24-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=u3n8l4hsal09u&continue=https://doc-0c-24-docs.googleusercontent.com/docs/securesc/n4i0jccn2khd3a24fmlud00hma5lfk85/ns4aak217ov5hek6mu4t7shc5d86meec/1640354250000/06548341279621459266/07306670677730082107Z/1phKUhamxVSa8f68FKgs5lOIUQMnlt58l?e%3Ddownload&hash=ddqicjt9org7eir65in1l46tal9a8pul [following]\n",
            "--2021-12-24 13:58:09--  https://docs.google.com/nonceSigner?nonce=u3n8l4hsal09u&continue=https://doc-0c-24-docs.googleusercontent.com/docs/securesc/n4i0jccn2khd3a24fmlud00hma5lfk85/ns4aak217ov5hek6mu4t7shc5d86meec/1640354250000/06548341279621459266/07306670677730082107Z/1phKUhamxVSa8f68FKgs5lOIUQMnlt58l?e%3Ddownload&hash=ddqicjt9org7eir65in1l46tal9a8pul\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.204.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0c-24-docs.googleusercontent.com/docs/securesc/n4i0jccn2khd3a24fmlud00hma5lfk85/ns4aak217ov5hek6mu4t7shc5d86meec/1640354250000/06548341279621459266/07306670677730082107Z/1phKUhamxVSa8f68FKgs5lOIUQMnlt58l?e=download&nonce=u3n8l4hsal09u&user=07306670677730082107Z&hash=vgktbvrqol426vtdsjl7pht2ua3ub7rg [following]\n",
            "--2021-12-24 13:58:10--  https://doc-0c-24-docs.googleusercontent.com/docs/securesc/n4i0jccn2khd3a24fmlud00hma5lfk85/ns4aak217ov5hek6mu4t7shc5d86meec/1640354250000/06548341279621459266/07306670677730082107Z/1phKUhamxVSa8f68FKgs5lOIUQMnlt58l?e=download&nonce=u3n8l4hsal09u&user=07306670677730082107Z&hash=vgktbvrqol426vtdsjl7pht2ua3ub7rg\n",
            "Connecting to doc-0c-24-docs.googleusercontent.com (doc-0c-24-docs.googleusercontent.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2527 (2.5K) [text/plain]\n",
            "Saving to: ‘evaluate.py’\n",
            "\n",
            "evaluate.py         100%[===================>]   2.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-24 13:58:11 (106 MB/s) - ‘evaluate.py’ saved [2527/2527]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "def read_dataset(data_path, read_fn=lambda x: json.loads(x), sep='\\t'):\n",
        "    vocab = defaultdict(list)\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line_split = line.replace(\"\\n\", '').split(sep)\n",
        "            word = line_split[0]\n",
        "            hypernyms = read_fn(line_split[1])\n",
        "            vocab[word].append(hypernyms)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "HlrWFpBkvBbp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = read_dataset(\"./nouns_en.2.0-3.0.tsv\")\n",
        "\n",
        "with open(\"no_labels_nouns_en.2.0-3.0.tsv\", 'r') as f:\n",
        "    new_words = f.read().split(\"\\n\")[:-1][:200]"
      ],
      "metadata": {
        "id": "o050fTyavyII"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_node2vec = {}\n",
        "\n",
        "for word in new_words:\n",
        "    predicted_node2vec[word] = [i[0] for i in sg_model.similar_by_vector(word2vec_vectors[word])]"
      ],
      "metadata": {
        "id": "2oqMrT-tv3f8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import get_score\n",
        "\n",
        "get_score(reference, predicted_node2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3SH7p9iv_Aj",
        "outputId": "68c7a0cc-060b-454e-9f8b-e5c225b850fa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.01125595238095238, 0.011791666666666667)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, v in list(predicted_node2vec.items())[:10]:\n",
        "    print(f\"word: {i}\")\n",
        "    print(f\"true: {reference[i]}\")\n",
        "    print(f\"predicted: {v}\")\n",
        "    print(\"=====\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Ciq0N-wcIx",
        "outputId": "87668888-0488-4d23-bd0d-816560de35b5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: sanguine\n",
            "true: [['chromatic_color.n.01', 'red.n.01']]\n",
            "predicted: ['petit_bourgeois.n.01', 'lay_reader.n.01', 'pip-squeak.n.01', 'bourgeois.n.02', 'plebeian.n.01', 'philistine.n.01', 'apiary.n.01', 'cipher.n.04', 'layman.n.01', 'muse.n.01']\n",
            "=====\n",
            "word: arccosine\n",
            "true: [['trigonometric_function.n.01', 'function.n.01']]\n",
            "predicted: ['anserinae.n.01', 'synodontidae.n.01', 'poeciliidae.n.01', 'dasyatis.n.01', 'peristediinae.n.01', 'ardeidae.n.01', 'toxotidae.n.01', 'artamidae.n.01', 'catostomidae.n.01', 'genyonemus.n.01']\n",
            "=====\n",
            "word: smoothbore\n",
            "true: [['firearm.n.01', 'gun.n.01']]\n",
            "predicted: ['weapon.n.01', 'firearm.n.01', 'gun.n.01', 'autoloader.n.01', 'knife.n.02', 'muzzle_loader.n.01', 'crossbow.n.01', 'semiautomatic_firearm.n.01', \"cupid's_bow.n.02\", 'garand_rifle.n.01']\n",
            "=====\n",
            "word: arthrodesis\n",
            "true: [['operation.n.06', 'arthroplasty.n.01']]\n",
            "predicted: ['sigmoidectomy.n.01', 'nephrectomy.n.01', 'oophorosalpingectomy.n.01', 'thyroidectomy.n.01', 'sympathectomy.n.01', 'prostatectomy.n.01', 'hypophysectomy.n.01', 'orchidectomy.n.01', 'pneumonectomy.n.01', 'splenectomy.n.01']\n",
            "=====\n",
            "word: sarsenet\n",
            "true: [['silk.n.01', 'fabric.n.01']]\n",
            "predicted: ['synodontidae.n.01', 'carapidae.n.01', 'batrachoididae.n.01', 'peristediinae.n.01', 'ophiodontidae.n.01', 'carangidae.n.01', 'cichlidae.n.01', 'podiceps.n.01', 'oxyura.n.01', 'agonidae.n.01']\n",
            "=====\n",
            "word: eutrophication\n",
            "true: [['process.n.06', 'organic_process.n.01']]\n",
            "predicted: ['lesseps.n.01', 'dinka.n.01', \"charge_d'affaires.n.01\", 'masai.n.01', 'luo.n.01', 'mandarin.n.03', 'padrone.n.01', 'nilotic.n.01', 'hull.n.04', 'hearing_examiner.n.01']\n",
            "=====\n",
            "word: noseband\n",
            "true: [['leather_strip.n.01', 'strap.n.01']]\n",
            "predicted: ['cloudlessness.n.01', 'highlight.n.02', 'light.n.07', 'dazzle.n.01', 'linen.n.03', 'crossbar.n.03', 'aura.n.02', 'opalescence.n.01', 'bay.n.07', 'livingston.n.01']\n",
            "=====\n",
            "word: comicality\n",
            "true: [['quality.n.01', 'humor.n.04']]\n",
            "predicted: ['dasyatis.n.01', 'myxocephalus.n.01', 'catostomus.n.01', 'sciaena.n.01', 'fundulus.n.01', 'brama.n.01', 'genus_tarpon.n.01', 'ocyurus.n.01', 'onchorynchus.n.01', 'astropogon.n.01']\n",
            "=====\n",
            "word: fanjet\n",
            "true: [['airplane.n.01', 'jet.n.01'], ['reaction-propulsion_engine.n.01', 'jet_engine.n.01']]\n",
            "predicted: ['synodontidae.n.01', 'carapidae.n.01', 'engraulidae.n.01', 'carangidae.n.01', 'poeciliidae.n.01', 'cichlidae.n.01', 'kyphosidae.n.01', 'agonidae.n.01', 'batrachoididae.n.01', 'ophidiidae.n.01']\n",
            "=====\n",
            "word: posturer\n",
            "true: [['organism.n.01', 'causal_agent.n.01', 'person.n.01']]\n",
            "predicted: ['catostomus.n.01', 'genus_tarpon.n.01', 'dasyatis.n.01', 'sciaena.n.01', 'galeocerdo.n.01', 'thunnus.n.01', 'astropogon.n.01', 'albula.n.01', 'myxocephalus.n.01', 'stizostedion.n.01']\n",
            "=====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv = KeyedVectors.load_word2vec_format(\"./nouns_w2v_all_2.0-3.0.txt\")"
      ],
      "metadata": {
        "id": "p5IoMQG2wueH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_predict = {}\n",
        "\n",
        "for test_name in new_words:\n",
        "    wv_predict[test_name] = [i[0] for i in wv.similar_by_word(test_name)]"
      ],
      "metadata": {
        "id": "j_4kJ2U4_QaB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_score(reference, wv_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b44ok7mW_bbV",
        "outputId": "8553e868-34b4-41ce-bf09-0792e917710b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.022936507936507936, 0.024186507936507937)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn_names = [i.name() for i in wn.all_synsets()]"
      ],
      "metadata": {
        "id": "q227ZIDZ_fAC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HCHModel:\n",
        "    def __init__(self, wv):\n",
        "        self.w2v_synsets = wv\n",
        "\n",
        "    def compute_candidates(self, neologism, topn=10):\n",
        "        return self.compute_hchs(neologism, topn=10)[:topn]\n",
        "\n",
        "    def compute_hchs(self, neologism, topn=100) -> list:\n",
        "        associates = map(itemgetter(0), self.generate_associates(neologism, topn))\n",
        "        associates = [i for i in associates if '.n.' in i and i in wn_names]\n",
        "        hchs = [hypernym.name() for associate in associates for hypernym in wn.synset(associate).hypernyms()]\n",
        "        return hchs\n",
        "    \n",
        "    def generate_associates(self, neologism, topn=10) -> list:\n",
        "        return self.w2v_synsets.similar_by_word(neologism, topn)\n"
      ],
      "metadata": {
        "id": "-uYgOC0G_h6t"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "hch = HCHModel(wv)"
      ],
      "metadata": {
        "id": "I_jxPaSb_kWc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_predict = {}\n",
        "\n",
        "for test_name in new_words:\n",
        "    wv_predict[test_name] = hch.compute_candidates(test_name)\n",
        "    \n",
        "get_score(reference, wv_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsbzG8dX_m4Q",
        "outputId": "2ffa98e6-9126-4e60-e87a-4628fea1dd58"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0874781746031746, 0.08976984126984126)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset(\"gal.n.01\").hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAaYyI0Y_pRL",
        "outputId": "5a6b05e0-43ba-444a-de30-cf628d0d96e8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('united_states_liquid_unit.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hch.generate_associates(\"camera.n.01\")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CP6tmsRMHnS",
        "outputId": "e3ecf9bd-92a7-47cf-fc05-b6598675bce2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('television_camera.n.01', 0.9472144246101379),\n",
              " ('camera_obscura.n.01', 0.9435300827026367),\n",
              " ('camera_lucida.n.01', 0.9417228102684021),\n",
              " ('camera_tripod.n.01', 0.8724722266197205),\n",
              " ('flash_camera.n.01', 0.856145977973938),\n",
              " ('digital_camera.n.01', 0.8518924117088318),\n",
              " ('portrait_camera.n.01', 0.8179097175598145),\n",
              " ('sound_camera.n.01', 0.8039113283157349),\n",
              " ('camera_angle.n.01', 0.7917401790618896),\n",
              " ('candid_camera.n.01', 0.7882478833198547)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hch.compute_candidates(\"camera.n.01\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WmzYfGnL7jF",
        "outputId": "ad9e3d91-bd56-4920-c8de-9a7f4c43233e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['television_equipment.n.01',\n",
              " 'chamber.n.01',\n",
              " 'optical_device.n.01',\n",
              " 'tripod.n.01',\n",
              " 'camera.n.01',\n",
              " 'camera.n.01',\n",
              " 'camera.n.01',\n",
              " 'motion-picture_camera.n.01',\n",
              " 'point_of_view.n.02',\n",
              " 'camera.n.01']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Results and Discussion\n",
        "\n",
        "1) Training Data in SemEval-2015 Task 17 missing so I used the test set attached in rar file.\\\n",
        "2) The mAP score for the new_word set from our trained node2vec model = 0.01125595238095238 while after changing similar words from nouns_w2v_all_2.0-3.0.txt the score was 0.022936507936507936\\\n",
        "3) Final mAP score for top 10 synonyms and their hypernyms = 0.0874781746031746\\\n",
        "4) the mRR score fot the new_word set from out trained model = 0.011791666666666667 while after changing similar words from nouns_w2v_all_2.0-3.0.txt the score was 0.024186507936507937\\\n",
        "5) 3) Final mRR score for top 10 synonyms and their hypernyms = 0.08976984126984126\\"
      ],
      "metadata": {
        "id": "l5LrRFkGmBrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tZRFAZSCpJpN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}